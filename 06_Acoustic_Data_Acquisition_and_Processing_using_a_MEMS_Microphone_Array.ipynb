{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "def css_styling():\n",
    "    styles = open(\"./custom.css\", \"r\").read()\n",
    "    return HTML(styles)\n",
    "css_styling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger(\"bokeh.embed.util\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import acoular as ac\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/DAGA_logo.png\" alt=\"DAGA Logo\" style=\"width:160px; position:absolute; top:20px; right:0px;\">\n",
    "<img src=\"img/TU-lang.png\" alt=\"TU Logo\" style=\"width:230px; position:absolute; top:0px; right:180px;\">\n",
    "\n",
    "<h1 style=\"margin-top: 180px; margin-bottom: 50px; color: #A81D1E;\">Acoustic Data Acquisition and Processing in Python using a MEMS Microphone Array</h1>\n",
    "<h3 style=\"margin-top: 50px; margin-bottom: 50px; color: #434343;\">Rabea Eschenhagen, Adam Kujawski, Art J. R. Pelling, Mikolaj Czuchaj, Gert Herold, Oliver Lylloff, Ennes Sarradj</h3>\n",
    "\n",
    "<h5 style=\"margin-top: 50px; margin-bottom: 100px; color: #434343;\">Jupyter Notebook available at <a href=\"https://github.com/acoular/workshop_DAS-DAGA-2025\">https://github.com/acoular/workshop_DAS-DAGA-2025</a></h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## What we will do\n",
    "\n",
    "- Stream and process live audio data from the laptop's microphone\n",
    "- Revisit a simple beamforming example using microphone array data\n",
    "- Demonstrate an application that integrates the discussed concepts: Acoular extension _SpectAcoular_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting data from input channels\n",
    "\n",
    "### Sounddevice library\n",
    "- Current environment must include the `sounddevice` library\n",
    "- First step: Check the available audio devices on the system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "\n",
    "sd.query_devices() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set index of the desired audio device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_number = None #TODO: Set the device number here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Acoular's SoundDeviceSamplesGenerator\n",
    "\n",
    "- `SoundDeviceSamplesGenerator` class handles audio data streams\n",
    "- Uses the sounddevice library internally\n",
    "- Requires setting the device index to choose the audio interface\n",
    "- Configuration for single-channel acquisition (numchannels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = ac.SoundDeviceSamplesGenerator(\n",
    "    device = device_number,\n",
    "    numchannels = 1,\n",
    ")\n",
    "\n",
    "print(\"Sampling frequency: \", dev.sample_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform voltages into sound pressure \n",
    "\n",
    "- Recorded signal is in volts and needs conversion to sound pressure (Pa)\n",
    "- Sensitivity defines voltage output per unit of sound pressure\n",
    "- Conversion is done by multiplying with the reciprocal of sensitivity\n",
    "- _Example: 0.016 V/Pa means 1 Pa corresponds to 0.016 V_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_calibrated = ac.Calib(source=dev, data=1/0.016*np.ones(dev.num_channels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Record a signal\n",
    "\n",
    "- Acquire a signal with a duration of 2 seconds\n",
    "- Read input stream block by block (block size: 256 samples)\n",
    "- Use `return_result` to retrieve all blocks at once\n",
    "- `SoundDeviceSamplesGenerator` prints device properties when streaming starts\n",
    "- Array `t`: The first axis is time data, and the second axis is the number of channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recording_time = 2 # seconds \n",
    "dev.num_samples = int(recording_time * dev.sample_freq)\n",
    "\n",
    "signal = ac.tools.return_result(dev_calibrated, num=256)\n",
    "t = np.arange(signal.shape[0]) / dev.sample_freq "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot of results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,4))\n",
    "plt.plot(t, signal[:,0])\n",
    "plt.ylabel('Sound Pressure [Pa]')\n",
    "plt.xlabel('Time [s]')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing measurement data to disk\n",
    "\n",
    "### Further Processing\n",
    "\n",
    "- Acoular allows additional processing steps beyond acquiring time data\n",
    "- Processing is handled via classes from the `tprocess.py` module\n",
    "- `WriteH5` class saves the stream to an HDF5 file\n",
    "- Alternatively, `WriteWav` can be used to save the stream as a WAV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5 = ac.WriteH5(\n",
    "    source = dev_calibrated,\n",
    "    file = 'sounddevice.h5',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to disk\n",
    "- Two processing chains were created to read data from `dev` (`SoundeDeviceSamplesGenerator`)\n",
    "- Data can either be passed on or saved to disk\n",
    "- Due to lazy evaluation, no processing has occurred yet\n",
    "- Two ways to start acquisition and writing:\n",
    "\n",
    "    1. `hdf5.result(num=256)`: Generates data block-wise\n",
    "    2. `hdf5.save()`: Runs the processing chain in one go without returning data\n",
    "- Option 2 is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read file\n",
    "\n",
    "- File can be processed with any software supporting this format\n",
    "- Use the `TimeSamples` object to read the data for further post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = ac.TimeSamples(file='sounddevice.h5')\n",
    "\n",
    "print(\"Number of channels: \", ts.num_channels, \"; Number of samples: \", ts.num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Microphone array data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up input device parameters\n",
    "- Set up input device parameters for recording.\n",
    "- A 16-channel low-cost microphone array was used for recording\n",
    "- `SoundDeviceSamplesGenerator` initialized with:\n",
    "    - `device = 0`\n",
    "    - `numchannels = 16`\n",
    "- Recording duration: 4 seconds.\n",
    "- Multichannel data saved in `img/multichannel_signal.h5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_data = ac.MaskedTimeSamples(file='img/multichannel_signal.h5')\n",
    "\n",
    "print(\"Number of channels: \", time_data.num_channels, \"; Number of samples: \", time_data.num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the microphone geometry and the computational grid\n",
    "-   Microphone array geometry is loaded from an XML file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mics = ac.MicGeom(file=ac.__path__[0] + '/xml/minidsp_uma-16_mirrored.xml')\n",
    "\n",
    "# Plot of microphone positions\n",
    "plt.scatter(*mics.pos[:2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a rectangular grid \n",
    "- Specifies the search region for sound sources\n",
    "- Covers the spatial range with a resolution of 0.05 m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_min, x_max = -1.2, 1.2\n",
    "y_min, y_max = -1.2, 1.2\n",
    "z = 1.54\n",
    "increment = 0.05\n",
    "\n",
    "grid = ac.RectGrid(                 # for 3D Grid use ac.RectGrid3D and set z_min and z_max\n",
    "    x_min=x_min, x_max=x_max, \n",
    "    y_min=y_min, y_max=y_max, \n",
    "    z=z, \n",
    "    increment=increment\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the steering vector for beamforming\n",
    "\n",
    "- Set the speed of sound in air\n",
    "- Create a steering vector using microphone geometry and a defined rectangular grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 346 # speed of sound\n",
    "steer = ac.SteeringVector(mics=mics, env=ac.Environment(c=c), grid=grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup beamforming\n",
    "- Transform the recorded time-domain signal into the frequency-domain\n",
    "- Apply Hanning window with 50% overlap\n",
    "- Use a block size of 128 samples\n",
    "- `BeamformerBase` performs beamforming using the delay-and-sum algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_data = ac.PowerSpectra(source=time_data, window='Hanning', overlap='50%', block_size=128)\n",
    "bb = ac.BeamformerBase(freq_data=freq_data, steer=steer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply beamforming \n",
    "- _Example_: Use a third-octave frequency band centred at 4000 Hz\n",
    "- Convert raw beamforming output to sound pressure levels (SPL) in decibels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency = 4000 \n",
    "result = bb.synthetic(frequency, num=3)\n",
    "Lm = ac.L_p(result)\n",
    "Lm_reshaped = Lm.reshape(grid.shape) # converting 1D array to 2D array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot of beamforming results \n",
    "- Overlay results onto a background image\n",
    "- Heatmap represents sound pressure levels (SPL) in decibels\n",
    "- SPL values are mapped to image coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Background image \n",
    "background_img = plt.imread(\"img/multichannel_signal.jpg\") \n",
    "im_w, im_h = background_img.shape[1], background_img.shape[0] \n",
    "ax.imshow(background_img, extent=[0, im_w, 0, im_h])\n",
    "\n",
    "# Pixel indices of Beamforming plot in photo for correct placing\n",
    "p_ind = lambda img, v: (img / 2 + 230 * (v / z)) \n",
    "\n",
    "# Beamforming plot\n",
    "heatmap = ax.imshow(\n",
    "    Lm_reshaped, extent=[p_ind(im_w, x_min), p_ind(im_w, x_max), p_ind(im_h, y_min), p_ind(im_h, y_max)], \n",
    "    origin='lower', cmap='jet', alpha=0.4, vmin=70, vmax=80\n",
    ")\n",
    "\n",
    "cbar = fig.colorbar(heatmap, ax=ax)\n",
    "cbar.set_label(\"Sound Pressure Level (dB)\")\n",
    "\n",
    "ax.set_xlim([0, im_w])\n",
    "ax.set_ylim([0, im_h])\n",
    "\n",
    "ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SpectAcoular! Extension for Acoular\n",
    "### What it is\n",
    "- Easy and interactive microphone array data processing\n",
    "- Designed for microphone array data acquisition, analysis, and exploration\n",
    "- Provides interactive tools for real-time beamforming and signal processing\n",
    "- Implements graphical controls (widgets) for user interaction\n",
    "- Customizable and expandable for different applications\n",
    "\n",
    "### Where to find it\n",
    "- **GitHub:** [SpectAcoular repository](https://github.com/acoular/spectacoular)\n",
    "- **Anaconda:** [SpectAcoular package](https://anaconda.org/acoular/spectacoular)\n",
    "- **Documentation:** [SpectAcoular docs](https://acoular.github.io/spectacoular)\n",
    "  \n",
    "### Installation\n",
    "- With Conda: `conda install acoular::spectacoular`\n",
    "\n",
    "### How it works\n",
    "- Uses [_Bokeh_](https://bokeh.org/) for web-based visualization, allowing client-server applications\n",
    "- Translates class attributes from Acoular into interactive control widgets\n",
    "- Allows real-time parameter adjustments directly from the user interface\n",
    "- _Example:_ Creating an Interactive Grid, as shown below\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Ensure that Bokeh and SpectAcoular are installed in the current environment.\n",
    "# If these packages are not available, you may skip this cell.\n",
    "\n",
    "from spectacoular import RectGrid\n",
    "from bokeh.io import show, output_notebook\n",
    "from bokeh.layouts import column\n",
    "\n",
    "grid = RectGrid()\n",
    "widgets = grid.get_widgets()\n",
    "output_notebook()\n",
    "widget_list = list(widgets.values())\n",
    "col = column(widget_list, sizing_mode=\"scale_both\", width=200)\n",
    "\n",
    "show(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-build interactive applications\n",
    "\n",
    "- Module provides various predefined examples and applications\n",
    "  - to explore the point-spread function of a microphone arrays\n",
    "  - to evaluate rotating array data\n",
    "  - to measure sound sources\n",
    "  - ...\n",
    "- As an example, we will take a look at the [Microphone array measurement app](https://acoular.github.io/spectacoular/apps/measurementapp.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.popen('msm_app --args --device=sounddevice')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acoular Future Directions Survey â€“ Don't forget to participate! \n",
    "\n",
    "<img src=\"img/survey.png\" alt=\"Survey\" style=\"height: auto; width: 50%;\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
